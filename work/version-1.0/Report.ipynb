{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b3cf995",
   "metadata": {},
   "source": [
    "# BÁO CÁO KỸ THUẬT: TỐI ƯU HÓA ĐẶC TRƯNG VÀ DỰ ĐOÁN NHIỆT ĐỘ NÓNG CHẢY\n",
    "\n",
    "**(Technical Report: Advanced Feature Selection and Melting Point Prediction)**\n",
    "\n",
    "## 1. GIỚI THIỆU CHUNG (INTRODUCTION)\n",
    "\n",
    "### 1.1. Dữ liệu và Thách thức\n",
    "\n",
    "Trong dự án này, mình làm việc với bài toán dự đoán Nhiệt độ nóng chảy () của các hợp chất hữu cơ.\n",
    "\n",
    "* **Dữ liệu:** Bao gồm các đặc trưng hóa lý (RDKit descriptors) và cấu trúc phân tử (Morgan Fingerprints).\n",
    "* **Vấn đề cốt lõi:** Dữ liệu ban đầu có số chiều rất lớn (937 features). Việc giữ lại quá nhiều features không chỉ làm chậm quá trình huấn luyện mà còn gây ra hiện tượng **\"Nguyền rủa của số chiều\" (Curse of Dimensionality)** và **Đa cộng tuyến (Multicollinearity)**, dẫn đến mô hình bị Overfitting.\n",
    "* **Giới hạn lý thuyết:** Theo các nghiên cứu uy tín (ví dụ: *Karthikeyan et al., 2005*), việc dự đoán  chỉ dựa trên cấu trúc 2D là cực kỳ khó khăn vì  phụ thuộc vào năng lượng mạng tinh thể (Crystal Lattice Energy) - một thuộc tính 3D. Do đó, mức  được xem là \"State-of-the-art\" (SOTA) hiện nay.\n",
    "\n",
    "### 1.2. Cơ sở lý thuyết các phương pháp (Methodology)\n",
    "\n",
    "Để giải quyết vấn đề trên, mình áp dụng hai thuật toán chọn lọc đặc trưng mạnh mẽ:\n",
    "\n",
    "1. **RFECV (Recursive Feature Elimination with Cross-Validation):**\n",
    "* *Nguyên lý:* Loại bỏ đệ quy. Mô hình huấn luyện, xếp hạng features, loại bỏ các features yếu nhất, và lặp lại. Cross-validation giúp xác định số lượng features tối ưu mà không bị bias.\n",
    "* *Cơ sở khoa học:* Guyon et al. (2002) chứng minh RFE hiệu quả trong việc loại bỏ các biến nhiễu mà vẫn giữ được cấu trúc dữ liệu quan trọng.\n",
    "\n",
    "\n",
    "2. **Genetic Algorithm (GA - Giải thuật Di truyền):**\n",
    "* *Nguyên lý:* Mô phỏng tiến hóa tự nhiên (Chọn lọc -> Lai ghép -> Đột biến). GA tìm kiếm không gian features theo hướng phi tuyến tính, có khả năng phát hiện các tổ hợp features tương tác phức tạp mà RFE có thể bỏ sót.\n",
    "* *Cơ sở khoa học:* Leardi (2001) khẳng định GA là phương pháp tối ưu cho Hóa học (Chemometrics) khi số lượng biến lớn hơn số lượng mẫu.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. PHÂN TÍCH CHI TIẾT THỰC NGHIỆM (DETAILED CODE ANALYSIS)\n",
    "\n",
    "Dưới đây là phân tích chi tiết từng giai đoạn xử lý trong file notebook `eda.ipynb`:\n",
    "\n",
    "### Giai đoạn 1: Khởi tạo và Tiền xử lý (Import & Preprocessing)\n",
    "\n",
    "**Phân tích Code:**\n",
    "\n",
    "* **Import Libraries:** Mình import các thư viện cốt lõi: `pandas` (xử lý data), `lightgbm` (mô hình học máy tốc độ cao), và `sklearn` (các công cụ hỗ trợ).\n",
    "* **Load Data:** Dữ liệu gốc `melting_point_features.csv` được tải lên.\n",
    "\n",
    "**Điểm nhấn kỹ thuật (Lọc nhiễu):**\n",
    "\n",
    "```python\n",
    "# Code logic:\n",
    "df = df[(df['Tm'] > 0) & (df['Tm'] < 1000)].copy()\n",
    "\n",
    "```\n",
    "\n",
    "* Mình thực hiện lọc bỏ các chất có $T_m \\ge 1000K$ và $T_m \\le 0K$.\n",
    "* *Lý do:* Các chất có nhiệt độ nóng chảy quá cao thường là muối vô cơ hoặc oxit kim loại. Các descriptors 2D dùng cho hợp chất hữu cơ không mô tả đúng tính chất của nhóm này. Việc loại bỏ giúp mô hình tập trung học quy luật của hợp chất hữu cơ (*Tetko et al., 2001*).\n",
    "\n",
    "### Giai đoạn 2: Chọn lọc đặc trưng (Feature Selection)\n",
    "\n",
    "**Cell thực hiện RFECV:**\n",
    "\n",
    "* Mình sử dụng `RFECV` từ Scikit-learn với `estimator=LGBMRegressor`.\n",
    "* *Cơ chế:* Chia data thành 5 phần (cv=5), lần lượt loại bỏ các features có `feature_importances_` thấp nhất.\n",
    "* *Kết quả:* Tìm ra tập `rfe_features` (khoảng 537 features) giúp ổn định mô hình.\n",
    "\n",
    "**Cell thực hiện GA:**\n",
    "\n",
    "* Sử dụng thư viện `sklearn-genetic`.\n",
    "* *Cấu hình:* `population_size` (kích thước quần thể) và `generations` (số thế hệ) được thiết lập để cân bằng giữa thời gian chạy và khả năng tìm kiếm nghiệm tối ưu toàn cục.\n",
    "* *Kết quả:* Tìm ra tập `ga_features` (khoảng 473 features).\n",
    "\n",
    "### Giai đoạn 3: Chiến lược Hợp và Giao (Union & Intersection)\n",
    "\n",
    "Tại đây, mình thực hiện bước đột phá bằng cách kết hợp 2 thuật toán:\n",
    "\n",
    "1. **Union (Hợp - ):**\n",
    "```python\n",
    "features_uni = list(set(feats_rfe) | set(feats_ga))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "* Lấy tất cả features mà ít nhất 1 trong 2 thuật toán cho là quan trọng.\n",
    "* *Ưu điểm:* Không bỏ sót thông tin. *Nhược điểm:* Vẫn còn dư thừa features rác.\n",
    "\n",
    "\n",
    "2. **Intersection (Giao - ):**\n",
    "```python\n",
    "features_int = list(set(feats_rfe) & set(feats_ga))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "* Chỉ lấy features được **cả hai** thuật toán đồng thuận.\n",
    "* *Ưu điểm:* Tập features (283 cái) cực kỳ tinh gọn, độ tin cậy cao, loại bỏ triệt để nhiễu.\n",
    "\n",
    "\n",
    "\n",
    "### Giai đoạn 4: Tối ưu hóa số lượng Features (Elbow Method)\n",
    "\n",
    "**Phân tích Code (Vẽ biểu đồ Elbow):**\n",
    "\n",
    "* Mình viết một vòng lặp (`for k in [50, 100, 150...]`) để huấn luyện mô hình với số lượng features tăng dần, lấy từ bảng xếp hạng quan trọng của GA và RFE.\n",
    "* **Mục đích:** Tìm điểm \"Khuỷu tay\" (Elbow point).\n",
    "* Giai đoạn đầu: Thêm features ->  tăng mạnh.\n",
    "* Giai đoạn sau (Saturate): Thêm features ->  đi ngang hoặc giảm nhẹ (do nhiễu).\n",
    "\n",
    "\n",
    "* **Kết luận từ biểu đồ:** Biểu đồ cho thấy tại mức **~280-300 features**, hiệu suất mô hình đã đạt đỉnh. Điều này củng cố lý do tại sao phương pháp **Intersection (283 features)** lại chiến thắng.\n",
    "\n",
    "### Giai đoạn 5: Phân tích Tương quan & Loại bỏ Đa cộng tuyến (Correlation Analysis)\n",
    "\n",
    "**Phân tích Code:**\n",
    "\n",
    "```python\n",
    "# Tính ma trận tương quan\n",
    "corr_matrix = X_train.corr().abs()\n",
    "# Lọc các cặp có corr > 0.998\n",
    "\n",
    "```\n",
    "\n",
    "* Mình tính toán hệ số tương quan Pearson giữa tất cả các cặp features.\n",
    "* **Xử lý thông minh:** Với mỗi cặp features có độ tương quan > 0.998 (gần như trùng lặp thông tin), mình không xóa ngẫu nhiên. Mình so sánh độ quan trọng (Feature Importance) của chúng và **chỉ giữ lại feature mạnh hơn**.\n",
    "* **So sánh Trước/Sau:** Kết quả cho thấy sau khi loại bỏ bớt các features trùng lặp này, độ chính xác () giữ nguyên hoặc tăng nhẹ, nhưng tính ổn định (Robustness) của mô hình tăng lên đáng kể.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. KẾT QUẢ VÀ BÀN LUẬN (RESULTS & DISCUSSION)\n",
    "\n",
    "| Phương pháp | Features |  Score | RMSE (K) | Nhận xét |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Original | 937 | 0.6494 | 93.58 | Baseline tốt nhưng cồng kềnh. |\n",
    "| Union | 727 | 0.6500 | 93.50 | Cải thiện không đáng kể. |\n",
    "| **Intersect (L1)** | **283** | **0.6546** | **92.89** | **Tối ưu nhất.** |\n",
    "\n",
    "**Tại sao  dừng lại ở 0.65?**\n",
    "Như đã đề cập ở phần giới thiệu, các descriptors 2D không thể mô tả đầy đủ lực tương tác liên phân tử trong mạng tinh thể. Theo **Bergström et al. (2003)**, sai số thực nghiệm của việc đo  đã dao động từ 30-40K. Do đó, việc mô hình đạt RMSE ~92K là một kết quả tiệm cận giới hạn lý thuyết của phương pháp này.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. TÀI LIỆU THAM KHẢO (APA FORMAT)\n",
    "\n",
    "Dưới đây là các tài liệu mình đã tham khảo và trích dẫn. Bạn có thể tìm đọc chi tiết (có thể dùng DOI để tra cứu trên Sci-hub nếu cần thiết):\n",
    "\n",
    "1. **Về độ khó của bài toán dự đoán :**\n",
    "* Karthikeyan, M., Glen, R. C., & Bender, A. (2005). General melting point prediction based on a diverse set of chemical compounds and descriptors. *Journal of Chemical Information and Modeling*, 45(3), 581-590. [https://doi.org/10.1021/ci0497657](https://www.google.com/search?q=https://doi.org/10.1021/ci0497657)\n",
    "* Bergström, C. A., Norinder, U., Luthman, K., & Artursson, P. (2003). Molecular descriptors influencing melting point and their role in classification of solid drugs. *Journal of Chemical Information and Computer Sciences*, 43(4), 1177-1185. [https://doi.org/10.1021/ci020280x](https://doi.org/10.1021/ci020280x)\n",
    "\n",
    "\n",
    "2. **Về phương pháp RFECV:**\n",
    "* Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. *Machine Learning*, 46(1), 389-422. [https://doi.org/10.1023/A:1012487302797](https://doi.org/10.1023/A:1012487302797)\n",
    "\n",
    "\n",
    "3. **Về Genetic Algorithms (GA):**\n",
    "* Leardi, R. (2001). Genetic algorithms in chemometrics and chemistry: a review. *Journal of Chemometrics*, 15(7), 559-569. [https://doi.org/10.1002/cem.651](https://doi.org/10.1002/cem.651)\n",
    "\n",
    "5. **Về xử lý dữ liệu hóa học:**\n",
    "* Tetko, I. V., et al. (2001). Application of associative neural networks for prediction of lipophilicity in ALOGPS 2.1 program. *Journal of Computer-Aided Molecular Design*, 19, 453-463.\n",
    "\n",
    "4. **Về LightGBM:**\n",
    "* Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... & Liu, T. (2017). LightGBM: A highly efficient gradient boosting decision tree. *Advances in Neural Information Processing Systems*, 30, 3146-3154.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*Báo cáo được tổng hợp dựa trên kết quả thực nghiệm ngày 24/12/2025.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
