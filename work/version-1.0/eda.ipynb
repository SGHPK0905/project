{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba85359e",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a22807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "\n",
    "y = df['Tm']\n",
    "X = df.drop(columns=['Tm'])\n",
    "\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=2601)\n",
    "\n",
    "base_model = LGBMRegressor(random_state=2601, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad834c6e",
   "metadata": {},
   "source": [
    "# REMOVE OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_outliers_advanced(df, col):\n",
    "    initial_len = len(df)\n",
    "    df = df[(df[col] > 0) & (df[col] < 4000)].copy()\n",
    "    print(f\"ƒê√£ x√≥a {initial_len - len(df)} m·∫´u phi v·∫≠t l√Ω (<=0 ho·∫∑c >4000).\")\n",
    "\n",
    "    log_data = np.log1p(df[col])\n",
    "    \n",
    "    Q1 = log_data.quantile(0.25)\n",
    "    Q3 = log_data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound_log = Q1 - 2.0 * IQR\n",
    "    upper_bound_log = Q3 + 2.0 * IQR\n",
    "    \n",
    "    lower_bound = np.expm1(lower_bound_log)\n",
    "    upper_bound = np.expm1(upper_bound_log)\n",
    "    \n",
    "    print(f\"Ng∆∞·ª°ng gi·ªØ l·∫°i: {lower_bound:.2f} ƒë·∫øn {upper_bound:.2f}\")\n",
    "\n",
    "    return df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "train_data = X_train.copy()\n",
    "train_data['Tm'] = y_train\n",
    "\n",
    "print(f\"Original Train Size: {len(train_data)}\")\n",
    "\n",
    "train_data_clean = remove_outliers_advanced(train_data, 'Tm')\n",
    "\n",
    "print(f\"Cleaned Train Size: {len(train_data_clean)}\")\n",
    "print(f\"Removed Total: {len(train_data) - len(train_data_clean)} Samp\")\n",
    "\n",
    "X_train_clean = train_data_clean.drop(columns=['Tm'])\n",
    "y_train_clean = train_data_clean['Tm']\n",
    "\n",
    "X_train = X_train_clean\n",
    "y_train = y_train_clean\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√É C·∫¨P NH·∫¨T TH√ÄNH C√îNG!\")\n",
    "print(f\"X_train shape m·ªõi: {X_train.shape}\")\n",
    "print(f\"y_train shape m·ªõi: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176a307",
   "metadata": {},
   "source": [
    "# RFECV Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n--- START RFECV ---\")\n",
    "start = time.time()\n",
    "\n",
    "model_rfe = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    random_state=2601,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "rfe = RFECV(\n",
    "    estimator=model_rfe, \n",
    "    min_features_to_select=50,\n",
    "    step=20,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "selected_rfe = X_train.columns[rfe.support_]\n",
    "print(f\"Time Run: {time.time() - start:.2f} s\")\n",
    "print(f\"RFECV Selected: {len(selected_rfe)} features\")\n",
    "\n",
    "joblib.dump(list(selected_rfe), 'result/rfe_features.pkl')\n",
    "\n",
    "eval_model = LGBMRegressor(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=2601,\n",
    "    n_jobs=-1, \n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "eval_model.fit(X_train[selected_rfe], y_train)\n",
    "\n",
    "y_pred_log = eval_model.predict(X_test[selected_rfe])\n",
    "\n",
    "y_pred_real = np.expm1(y_pred_log)\n",
    "y_test_real = np.expm1(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "r2 = r2_score(y_test_real, y_pred_real)\n",
    "\n",
    "print(\"\\n--- RESULT (REAL SCALE - KELVIN) ---\")\n",
    "print(f\"Features: {len(selected_rfe)}\")\n",
    "print(f\"MAE     : {mae:.4f} K\")\n",
    "print(f\"RMSE    : {rmse:.4f} K\")\n",
    "print(f\"R2      : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcada578",
   "metadata": {},
   "source": [
    "# GENETIC ALGORITHM (GA) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n--- üß¨ START RUN GENETIC ALGORITHM ---\")\n",
    "start = time.time()\n",
    "\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "model_ga = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1, \n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "ga = GAFeatureSelectionCV(\n",
    "    estimator=model_ga,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    population_size=50,    \n",
    "    generations=15,\n",
    "    mutation_probability=0.1,\n",
    "    crossover_probability=0.8,\n",
    "    keep_top_k=2,\n",
    "    elitism=True,\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "ga.fit(X_train, y_train_log)\n",
    "\n",
    "selected_ga = X_train.columns[ga.support_]\n",
    "\n",
    "print(f\"Time Run: {time.time() - start:.2f} s\")\n",
    "print(f\"\\n‚úÖ GA Chosen {len(selected_ga)} features\")\n",
    "joblib.dump(list(selected_ga), 'result/ga_features.pkl')\n",
    "\n",
    "print(\"\\n--- EVALUATING (REAL SCALE) ---\")\n",
    "eval_model = LGBMRegressor(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=2601,\n",
    "    n_jobs=-1, \n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "eval_model.fit(X_train[selected_ga], y_train_log)\n",
    "\n",
    "y_pred_log = eval_model.predict(X_test[selected_ga])\n",
    "\n",
    "y_pred_real = np.expm1(y_pred_log)\n",
    "y_test_real = np.expm1(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "r2 = r2_score(y_test_real, y_pred_real)\n",
    "\n",
    "print(\"\\n--- RESULT ---\")\n",
    "print(f\"Features: {len(selected_ga)}\")\n",
    "print(f\"MAE     : {mae:.4f} K\")\n",
    "print(f\"RMSE    : {rmse:.4f} K\")\n",
    "print(f\"R2      : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586459f5",
   "metadata": {},
   "source": [
    "# UNION 2 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "rfe_features = joblib.load('result/rfe_features.pkl')\n",
    "ga_features = joblib.load('result/ga_features.pkl')\n",
    "\n",
    "common_features = set(rfe_features) | set(ga_features)\n",
    "\n",
    "print(f\"\\nüíé T·ªïng features sau khi g·ªôp (Union): {len(common_features)}\")\n",
    "print(list(common_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878057cb",
   "metadata": {},
   "source": [
    "# CHOICE BEST FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "best_features = list(common_features)\n",
    "\n",
    "print(f\"‚úÖ ƒêang train model v·ªõi {len(best_features)} features...\")\n",
    "\n",
    "manual_params = {\n",
    "    'n_estimators': 5000,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 15,\n",
    "    'objective': 'regression_l1',\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': 1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "final_model = LGBMRegressor(**manual_params)\n",
    "\n",
    "final_model.fit(X_clean[best_features], y)\n",
    "\n",
    "# L∆∞u model v√† danh s√°ch features\n",
    "joblib.dump(final_model, 'result/final_melting_point_model.pkl')\n",
    "joblib.dump(best_features, 'result/final_features_list.pkl')\n",
    "\n",
    "print(\"üíæ ƒê√£ l∆∞u model v√† features th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc7df6",
   "metadata": {},
   "source": [
    "# SCORING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c831c1d",
   "metadata": {},
   "source": [
    "## Score Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7549c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import ast\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "model = joblib.load('result/final_melting_point_model.pkl')\n",
    "features = joblib.load('result/final_features_list.pkl')\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "\n",
    "needed_cols = list(features) + ['Tm']\n",
    "\n",
    "existing_cols = [c for c in needed_cols if c in df.columns]\n",
    "\n",
    "df_reduced = df[existing_cols].copy()\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "y = df_reduced['Tm']\n",
    "X = df_reduced.drop(columns=['Tm'])\n",
    "\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X = X.mask(X > 1e308, np.nan)\n",
    "\n",
    "print(\"‚öôÔ∏è(Imputing)...\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=2601)\n",
    "\n",
    "y_pred = model.predict(X_test[features])\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- üèÅ RESULT ---\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8109b",
   "metadata": {},
   "source": [
    "## Score each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5522a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "\n",
    "print(\"--- üìä TH·ªêNG K√ä D·ªÆ LI·ªÜU G·ªêC ---\")\n",
    "print(df['Tm'].describe())\n",
    "\n",
    "neg_count = (df['Tm'] <= 0).sum()\n",
    "print(f\"\\nS·ªë l∆∞·ª£ng m·∫´u c√≥ Tm <= 0: {neg_count} m·∫´u\")\n",
    "\n",
    "df = df.dropna(subset=['Tm'])\n",
    "\n",
    "df_clean = df[(df['Tm'] > 0) & (df['Tm'] < 4000)].copy()\n",
    "print(f\"‚úÖ D·ªØ li·ªáu sau khi l·ªçc Outliers v√¥ l√Ω: {len(df_clean)} (ƒê√£ x√≥a {len(df) - len(df_clean)} d√≤ng)\")\n",
    "\n",
    "y = df_clean['Tm']\n",
    "X = df_clean.drop(columns=['Tm']).select_dtypes(include=[np.number])\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "thresh = len(X) * 0.3 \n",
    "X = X.dropna(thresh=thresh, axis=1)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_test, y_train_log, y_test_log = train_test_split(X_clean, y_log, test_size=0.2, random_state=2601)\n",
    "\n",
    "def get_metrics(name, feature_list):\n",
    "    valid_feats = [f for f in feature_list if f in X_train.columns]\n",
    "\n",
    "    if not valid_feats: return {\"Method\": name, \"Features\": 0, \"RMSE\": 0, \"R2\": 0, \"MAE\": 0}\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "        random_state=2601,\n",
    "        n_estimators=5000,      \n",
    "        learning_rate=0.01,      \n",
    "        num_leaves=31,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.5\n",
    "    )\n",
    "    \n",
    "    callbacks = [lgb.early_stopping(stopping_rounds=200, verbose=False), lgb.log_evaluation(period=0)]\n",
    "\n",
    "    model.fit(X_train[valid_feats], y_train_log, \n",
    "              eval_set=[(X_test[valid_feats], y_test_log)],\n",
    "              eval_metric='rmse',\n",
    "              callbacks=callbacks)\n",
    "\n",
    "    y_pred_log = model.predict(X_test[valid_feats], num_iteration=model.best_iteration_)\n",
    "    \n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_test_real = np.expm1(y_test_log)\n",
    "\n",
    "    return {\n",
    "        \"Method\": name,\n",
    "        \"Features\": len(valid_feats),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test_real, y_pred)),\n",
    "        \"R2\": r2_score(y_test_real, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_test_real, y_pred)\n",
    "    }\n",
    "\n",
    "feats_all = list(X_train.columns)\n",
    "\n",
    "try:\n",
    "    feats_rfe = joblib.load('result/rfe_features.pkl')\n",
    "except:\n",
    "    feats_rfe = []\n",
    "\n",
    "try:\n",
    "    feats_ga = joblib.load('result/ga_features.pkl')\n",
    "except:\n",
    "    feats_ga = []\n",
    "\n",
    "results = []\n",
    "results.append(get_metrics(\"Original\", feats_all))\n",
    "results.append(get_metrics(\"RFE\", feats_rfe))\n",
    "results.append(get_metrics(\"GA\", feats_ga))\n",
    "\n",
    "df_res = pd.DataFrame(results)\n",
    "base_rmse = df_res.loc[0, 'RMSE']\n",
    "base_r2 = df_res.loc[0, 'R2']\n",
    "base_mae = df_res.loc[0, 'MAE']\n",
    "\n",
    "df_res['Diff_RMSE'] = df_res['RMSE'] - base_rmse\n",
    "df_res['Diff_R2'] = df_res['R2'] - base_r2\n",
    "df_res['Diff_MAE'] = df_res['MAE'] - base_mae\n",
    "\n",
    "print(\"\\n--- K·∫æT QU·∫¢ M·ªöI ---\")\n",
    "print(df_res.round(4))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(y, kde=True, color='blue').set_title(\"Ph√¢n ph·ªëi Tm g·ªëc\")\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_log, kde=True, color='green').set_title(\"Ph√¢n ph·ªëi Tm sau Log (Chu·∫©n h∆°n)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "df = df.dropna(subset=['Tm'])\n",
    "\n",
    "df = df[(df['Tm'] > 0) & (df['Tm'] < 4000)].copy()\n",
    "\n",
    "y_log_temp = np.log1p(df['Tm'])\n",
    "Q1 = y_log_temp.quantile(0.25)\n",
    "Q3 = y_log_temp.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound_log = Q1 - 2.0 * IQR\n",
    "upper_bound_log = Q3 + 2.0 * IQR\n",
    "\n",
    "lower_bound = np.expm1(lower_bound_log)\n",
    "upper_bound = np.expm1(upper_bound_log)\n",
    "\n",
    "df_clean = df[(df['Tm'] >= lower_bound) & (df['Tm'] <= upper_bound)].copy()\n",
    "\n",
    "print(f\"‚úÖ D·ªØ li·ªáu s·∫°ch cu·ªëi c√πng: {len(df_clean)} m·∫´u (ƒê√£ lo·∫°i b·ªè {len(df) - len(df_clean)} nhi·ªÖu)\")\n",
    "\n",
    "y = df_clean['Tm']\n",
    "X = df_clean.drop(columns=['Tm']).select_dtypes(include=[np.number])\n",
    "\n",
    "y = y.reset_index(drop=True)\n",
    "X = X.reset_index(drop=True)\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_final = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_log, test_size=0.2, random_state=2601)\n",
    "\n",
    "def get_metrics(name, feature_list):\n",
    "    valid_feats = [f for f in feature_list if f in X_train.columns]\n",
    "\n",
    "    if not valid_feats: \n",
    "        return {\"Method\": name, \"Features\": 0, \"R2\": 0, \"MAE\": 0, \"RMSE\": 0}\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "        random_state=2601,\n",
    "        n_estimators=5000,      \n",
    "        learning_rate=0.01,     \n",
    "        num_leaves=31,          \n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.7,   \n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1\n",
    "    )\n",
    "    \n",
    "    callbacks = [lgb.early_stopping(stopping_rounds=100, verbose=False), lgb.log_evaluation(period=0)]\n",
    "\n",
    "    model.fit(X_train[valid_feats], y_train, \n",
    "              eval_set=[(X_test[valid_feats], y_test)],\n",
    "              eval_metric='rmse',\n",
    "              callbacks=callbacks)\n",
    "\n",
    "    y_pred_log = model.predict(X_test[valid_feats], num_iteration=model.best_iteration_)\n",
    "    y_pred_real = np.expm1(y_pred_log)\n",
    "    y_test_real = np.expm1(y_test)\n",
    "\n",
    "    return {\n",
    "        \"Method\": name,\n",
    "        \"Features\": len(valid_feats),\n",
    "        \"R2\": r2_score(y_test_real, y_pred_real),\n",
    "        \"MAE\": mean_absolute_error(y_test_real, y_pred_real),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "    }\n",
    "\n",
    "feats_all = list(X_train.columns)\n",
    "\n",
    "try:\n",
    "    feats_rfe = joblib.load('result/rfe_features.pkl')\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file RFE, b·ªè qua.\")\n",
    "    feats_rfe = []\n",
    "\n",
    "try:\n",
    "    feats_ga = joblib.load('result/ga_features.pkl')\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file GA, b·ªè qua.\")\n",
    "    feats_ga = []\n",
    "\n",
    "results = []\n",
    "print(\"üöÄ ƒêang ch·∫°y ƒë√°nh gi√° Original...\")\n",
    "results.append(get_metrics(\"Original\", feats_all))\n",
    "\n",
    "if feats_rfe:\n",
    "    print(\"üöÄ ƒêang ch·∫°y ƒë√°nh gi√° RFE...\")\n",
    "    results.append(get_metrics(\"RFE\", feats_rfe))\n",
    "\n",
    "if feats_ga:\n",
    "    print(\"üöÄ ƒêang ch·∫°y ƒë√°nh gi√° GA...\")\n",
    "    results.append(get_metrics(\"GA\", feats_ga))\n",
    "\n",
    "df_res = pd.DataFrame(results)\n",
    "\n",
    "if not df_res.empty:\n",
    "    base_r2 = df_res.loc[0, 'R2']\n",
    "    base_mae = df_res.loc[0, 'MAE']\n",
    "    base_rmse = df_res.loc[0, 'RMSE']\n",
    "\n",
    "    df_res['Diff_R2'] = df_res['R2'] - base_r2\n",
    "    df_res['Diff_MAE'] = df_res['MAE'] - base_mae\n",
    "    df_res['Diff_RMSE'] = df_res['RMSE'] - base_rmse\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"üìä B·∫¢NG K·∫æT QU·∫¢ SO S√ÅNH (Tr√™n t·∫≠p Real Tm)\")\n",
    "    print(\"=\"*40)\n",
    "    print(df_res.round(4))\n",
    "    \n",
    "    common = set(feats_rfe) & set(feats_ga)\n",
    "    print(f\"\\nüíé Common Features ({len(common)}):\", list(common))\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c ghi nh·∫≠n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736270fd",
   "metadata": {},
   "source": [
    "# GridSearch Find Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60922e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': 4,\n",
    "        'random_state': 2601,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 40),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 50),\n",
    "    }\n",
    "    \n",
    "    if 'best_features' in globals():\n",
    "        features_to_use = list(best_features)\n",
    "    elif 'selected_ga' in globals():\n",
    "        features_to_use = list(selected_ga)\n",
    "    else:\n",
    "        features_to_use = list(X_train_clean.columns)\n",
    "\n",
    "    X_opt = X_train_clean[features_to_use]\n",
    "    y_opt = y_train_clean\n",
    "    \n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=2601)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_opt, y_opt):\n",
    "        X_tr, X_val = X_opt.iloc[train_idx], X_opt.iloc[val_idx]\n",
    "        y_tr, y_val = y_opt.iloc[train_idx], y_opt.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMRegressor(**param)\n",
    "        \n",
    "        pruning_callback = LightGBMPruningCallback(trial, \"l1\")\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr, y_tr, \n",
    "            eval_set=[(X_val, y_val)], \n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=20, verbose=False),\n",
    "                pruning_callback\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study.optimize(objective, n_trials=5, timeout=60)\n",
    "\n",
    "print('Best params:', study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709c9b7",
   "metadata": {},
   "source": [
    "# Elbow GA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 2000,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': -1,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': 1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "valid_ga_feats = [f for f in list(selected_ga) if f in X_train.columns]\n",
    "\n",
    "print(\"ƒêang x·∫øp h·∫°ng features...\")\n",
    "ranker = LGBMRegressor(**best_params)\n",
    "ranker.fit(X_train[valid_ga_feats], y_train)\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    'Feature': valid_ga_feats,\n",
    "    'Importance': ranker.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "sorted_feats = imp_df['Feature'].tolist()\n",
    "\n",
    "steps = list(range(len(sorted_feats), 99, -50)) + list(range(90, 0, -10))\n",
    "results = []\n",
    "\n",
    "print(f\"\\nB·∫Øt ƒë·∫ßu v√≤ng l·∫∑p c·∫Øt gi·∫£m features ({len(steps)} v√≤ng)...\")\n",
    "\n",
    "for k in steps:\n",
    "    current_feats = sorted_feats[:k]\n",
    "    \n",
    "    model = LGBMRegressor(**best_params)\n",
    "    model.fit(X_train[current_feats], y_train)\n",
    "    \n",
    "    y_pred_log = model.predict(X_test[current_feats])\n",
    "    \n",
    "    y_pred_real = np.expm1(y_pred_log)\n",
    "    y_test_real = np.expm1(y_test)\n",
    "\n",
    "    r2 = r2_score(y_test_real, y_pred_real)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "    mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "\n",
    "    print(f\"   -> D√πng {k:3d} features: R2 = {r2:.4f} | MAE = {mae:.2f} | RMSE = {rmse:.2f}\")\n",
    "    results.append({'Num_Features': k, 'R2': r2, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(by='Num_Features')\n",
    "df_leaderboard = df_results.sort_values(by='R2', ascending=False).reset_index(drop=True)\n",
    "csv_filename = 'result/feature_selection_results.csv'\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['R2'], marker='o', linewidth=2, color='blue')\n",
    "\n",
    "best_row = df_results.loc[df_results['R2'].idxmax()]\n",
    "plt.scatter(best_row['Num_Features'], best_row['R2'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"ƒê·ªânh: {best_row['R2']:.4f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['R2']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['R2']-0.01),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('Bi·ªÉu ƒë·ªì Elbow: Hi·ªáu qu·∫£ khi gi·∫£m d·∫ßn s·ªë l∆∞·ª£ng Features', fontsize=14)\n",
    "plt.xlabel('S·ªë l∆∞·ª£ng Features', fontsize=12)\n",
    "plt.ylabel('ƒê·ªô ch√≠nh x√°c (R2)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nB·∫¢NG X·∫æP H·∫†NG (SCORE GI·∫¢M D·∫¶N):\")\n",
    "print(df_leaderboard[['R2', 'Num_Features', 'MAE', 'RMSE']].head(10))\n",
    "\n",
    "print(\"\\nB·∫¢NG THEO TH·ª® T·ª∞ FEATURE (√çT -> NHI·ªÄU):\")\n",
    "print(df_results[['Num_Features', 'R2', 'MAE', 'RMSE']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13a8c2",
   "metadata": {},
   "source": [
    "# Elbow RFECV Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ae099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- START RFE WITH LEADERBOARD ---\")\n",
    "start = time.time()\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=model,\n",
    "    step=20,\n",
    "    cv=3,\n",
    "    scoring='r2', \n",
    "    min_features_to_select=50,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "r2_scores = rfecv.cv_results_['mean_test_score']\n",
    "n_scores = len(r2_scores)\n",
    "\n",
    "feature_counts = [50 + i * 20 for i in range(n_scores)]\n",
    "\n",
    "if len(feature_counts) > len(r2_scores):\n",
    "    feature_counts = feature_counts[:len(r2_scores)]\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'Num_Features': feature_counts,\n",
    "    'Score_R2': r2_scores\n",
    "})\n",
    "\n",
    "df_leaderboard = df_results.sort_values(by='Score_R2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "selected_rfecv = X_train.columns[rfecv.support_]\n",
    "print(f\"\\nTime Run: {time.time() - start:.2f} s\")\n",
    "print(f\"Best Number of Features: {rfecv.n_features_}\")\n",
    "print(f\"Best CV R2 Score: {df_leaderboard.iloc[0]['Score_R2']:.4f}\")\n",
    "\n",
    "print(\"\\nLEADERBOARD (DESCENDING SCORE):\")\n",
    "print(df_leaderboard.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nPROGRESS (BY FEATURE COUNT):\")\n",
    "print(df_results.head(10).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['Score_R2'], marker='o', color='green', linewidth=2)\n",
    "\n",
    "best_row = df_leaderboard.iloc[0]\n",
    "plt.scatter(best_row['Num_Features'], best_row['Score_R2'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"Best: {best_row['Score_R2']:.4f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['Score_R2']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['Score_R2']-0.005),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('RFECV Performance Curve', fontsize=14)\n",
    "plt.xlabel('Number of Features Selected', fontsize=12)\n",
    "plt.ylabel('Cross Validation Score (R2)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSelected Features List:\")\n",
    "print(list(selected_rfecv))\n",
    "\n",
    "y_pred_log = rfecv.predict(X_test)\n",
    "y_pred_real = np.expm1(y_pred_log)\n",
    "y_test_real = np.expm1(y_test)\n",
    "\n",
    "test_r2 = r2_score(y_test_real, y_pred_real)\n",
    "test_mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "\n",
    "print(\"\\n--- FINAL TEST EVALUATION (Best Features) ---\")\n",
    "print(f\"R2 Score : {test_r2:.4f}\")\n",
    "print(f\"MAE      : {test_mae:.4f} K\")\n",
    "print(f\"RMSE     : {test_rmse:.4f} K\")\n",
    "\n",
    "df_results.to_csv('result/rfecv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728a31e",
   "metadata": {},
   "source": [
    "# Compare before and after using partial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e18594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ULTRA_HIGH_CORR = 0.998\n",
    "\n",
    "print(\"‚è≥ ƒêang t√≠nh to√°n ma tr·∫≠n t∆∞∆°ng quan...\")\n",
    "corr_matrix = X_train.corr().abs()\n",
    "\n",
    "ranker_temp = LGBMRegressor(n_estimators=100, verbose=-1, random_state=2601)\n",
    "ranker_temp.fit(X_train, y_train)\n",
    "importances = pd.Series(ranker_temp.feature_importances_, index=X_train.columns)\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = []\n",
    "\n",
    "for column in upper.columns:\n",
    "    correlated_cols = upper.index[upper[column] > ULTRA_HIGH_CORR].tolist()\n",
    "    \n",
    "    if correlated_cols:\n",
    "        for other_col in correlated_cols:\n",
    "            if other_col in to_drop: continue\n",
    "            \n",
    "            imp_col = importances.get(column, 0)\n",
    "            imp_other = importances.get(other_col, 0)\n",
    "            \n",
    "            if imp_col < imp_other:\n",
    "                to_drop.append(column)\n",
    "                break \n",
    "            else:\n",
    "                to_drop.append(other_col)\n",
    "\n",
    "to_drop = list(set(to_drop))\n",
    "print(f\"‚úÇÔ∏è ƒê√£ t√¨m th·∫•y {len(to_drop)} features tr√πng l·∫∑p (Corr > {ULTRA_HIGH_CORR})\")\n",
    "\n",
    "if len(to_drop) > 0:\n",
    "    feats_filtered = [f for f in X_train.columns if f not in to_drop]\n",
    "    \n",
    "    print(f\"üöÄ ƒêang ch·∫°y ƒë√°nh gi√° l·∫°i v·ªõi {len(feats_filtered)} features...\")\n",
    "    res_filtered = get_metrics(f\"Filtered (Corr > {ULTRA_HIGH_CORR})\", feats_filtered)\n",
    "    \n",
    "    new_r2 = res_filtered['R2']\n",
    "    \n",
    "    old_r2 = df_res.loc[df_res['Method'] == 'Original', 'R2'].values[0]\n",
    "    \n",
    "    print(f\"\\n‚úÖ K·∫øt qu·∫£ R2 C≈© (Original): {old_r2:.4f}\")\n",
    "    print(f\"‚úÖ K·∫øt qu·∫£ R2 M·ªõi (Filtered): {new_r2:.4f}\")\n",
    "    \n",
    "    methods = ['Original', 'Filtered']\n",
    "    scores = [old_r2, new_r2]\n",
    "    colors = ['gray', 'green' if new_r2 >= old_r2 else 'red']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(methods, scores, color=colors, width=0.5)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.title(f'So s√°nh R2: Gi·ªØ nguy√™n vs L·ªçc T∆∞∆°ng quan (> {ULTRA_HIGH_CORR})', fontsize=14)\n",
    "    plt.ylabel('R2 Score (Real Scale)', fontsize=12)\n",
    "    plt.ylim(0, max(scores) + 0.1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    new_row = pd.DataFrame([res_filtered])\n",
    "    df_res = pd.concat([df_res, new_row], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ Kh√¥ng c√≥ features n√†o qu√° gi·ªëng nhau ƒë·ªÉ x√≥a.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "df = df.dropna(subset=['Tm'])\n",
    "\n",
    "df = df[(df['Tm'] > 0) & (df['Tm'] < 1000)].copy()\n",
    "\n",
    "print(f\"Data size after removing outliers (>1000K): {len(df)}\")\n",
    "\n",
    "y = df['Tm']\n",
    "X = df.drop(columns=['Tm']).select_dtypes(include=[np.number])\n",
    "\n",
    "y = y.reset_index(drop=True)\n",
    "X = X.reset_index(drop=True)\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_final = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_log, test_size=0.2, random_state=2601)\n",
    "\n",
    "corr_matrix = X_train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.999)]\n",
    "\n",
    "if to_drop:\n",
    "    X_train = X_train.drop(columns=to_drop)\n",
    "    X_test = X_test.drop(columns=to_drop)\n",
    "    print(f\"Dropped {len(to_drop)} high correlation features\")\n",
    "\n",
    "voting_model = LGBMRegressor(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=2000,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = voting_model.predict(X_test)\n",
    "y_pred_real = np.expm1(y_pred_log)\n",
    "y_test_real = np.expm1(y_test)\n",
    "\n",
    "new_r2 = r2_score(y_test_real, y_pred_real)\n",
    "mae_real = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse_real = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "\n",
    "print(\"\\n--- RESULT ---\")\n",
    "print(f\"R2 Score : {new_r2:.4f}\")\n",
    "print(f\"MAE      : {mae_real:.4f} K\")\n",
    "print(f\"RMSE     : {rmse_real:.4f} K\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test_real, y_pred_real, alpha=0.4, color='blue', s=15, label='Data')\n",
    "\n",
    "max_val = max(y_test_real.max(), y_pred_real.max())\n",
    "min_val = min(y_test_real.min(), y_pred_real.min())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Fit')\n",
    "\n",
    "plt.title(f'Evaluation (Tm < 1000K)\\nR2 = {new_r2:.4f} | MAE = {mae_real:.2f} K', fontsize=14)\n",
    "plt.xlabel('Actual Tm (K)', fontsize=12)\n",
    "plt.ylabel('Predicted Tm (K)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(voting_model, 'result/model/final_melting_point_model.pkl')\n",
    "joblib.dump(list(X_train.columns), 'result/model/final_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a40ab",
   "metadata": {},
   "source": [
    "### Ph√¢n t√≠ch K·∫øt qu·∫£ Th·ª±c nghi·ªám (Ph·∫°m vi )\n",
    "\n",
    "**1. K·∫øt qu·∫£ ƒê·ªãnh l∆∞·ª£ng:**\n",
    "Sau khi lo·∫°i b·ªè c√°c gi√° tr·ªã ngo·∫°i lai () v√† l·ªçc b·ªè c√°c ƒë·∫∑c tr∆∞ng t∆∞∆°ng quan cao, m√¥ h√¨nh ƒë·∫°t:\n",
    "\n",
    "* ****: M·ª©c ƒë·ªô gi·∫£i th√≠ch bi·∫øn thi√™n d·ªØ li·ªáu ·ªü m·ª©c trung b√¨nh kh√°.\n",
    "* ****: Sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh kho·∫£ng 65 ƒë·ªô.\n",
    "\n",
    "**2. ƒê√°nh gi√° Nguy√™n nh√¢n Bi·∫øn ƒë·ªông:**\n",
    "So v·ªõi th·ª≠ nghi·ªám tr√™n t·∫≠p d·ªØ li·ªáu to√†n ph·∫ßn (bao g·ªìm c·∫£ ch·∫•t v√¥ c∆°/mu·ªëi), c√°c ch·ªâ s·ªë n√†y ph·∫£n √°nh ch√≠nh x√°c hi·ªáu nƒÉng tr√™n nh√≥m h·ª£p ch·∫•t h·ªØu c∆°:\n",
    "\n",
    "* **Hi·ªán t∆∞·ª£ng H·∫°n ch·∫ø Ph·∫°m vi (Range Restriction):** Vi·ªác lo·∫°i b·ªè mi·ªÅn gi√° tr·ªã cao () l√†m gi·∫£m ph∆∞∆°ng sai t·ªïng th·ªÉ c·ªßa t·∫≠p d·ªØ li·ªáu. V·ªÅ m·∫∑t to√°n h·ªçc, ƒëi·ªÅu n√†y khi·∫øn  gi·∫£m t·ª± nhi√™n, ƒë√≤i h·ªèi m√¥ h√¨nh ph·∫£i c√≥ ƒë·ªô nh·∫°y cao h∆°n ƒë·ªÉ ph√¢n bi·ªát c√°c m·∫´u c√≥ nhi·ªát ƒë·ªô g·∫ßn nhau.\n",
    "* **ƒê·ªô ch√≠nh x√°c th·ª±c t·∫ø (Honest Baseline):** M·ª©c  l√† sai s·ªë th·ª±c t·∫ø khi lo·∫°i b·ªè ·∫£nh h∆∞·ªüng c·ªßa c√°c ƒëi·ªÉm d·ªØ li·ªáu c·ª±c ƒëoan (outliers). Bi·ªÉu ƒë·ªì ph√¢n t√°n cho th·∫•y hi·ªán t∆∞·ª£ng \"co v·ªÅ trung b√¨nh\" (regression to the mean), khi m√¥ h√¨nh c√≥ xu h∆∞·ªõng d·ª± ƒëo√°n th·∫•p h∆°n th·ª±c t·∫ø ·ªü v√πng nhi·ªát ƒë·ªô cao (600-800K) v√† cao h∆°n th·ª±c t·∫ø ·ªü v√πng nhi·ªát ƒë·ªô th·∫•p (<200K).\n",
    "\n",
    "**K·∫øt lu·∫≠n:** ƒê√¢y l√† k·∫øt qu·∫£ c∆° s·ªü (baseline) tin c·∫≠y cho b√†i to√°n d·ª± ƒëo√°n tr√™n h·ª£p ch·∫•t h·ªØu c∆°, lo·∫°i b·ªè ho√†n to√†n c√°c y·∫øu t·ªë g√¢y nhi·ªÖu ho·∫∑c \"·∫£o gi√°c th·ªëng k√™\" t·ª´ c√°c gi√° tr·ªã ngo·∫°i lai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8e1ac",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
